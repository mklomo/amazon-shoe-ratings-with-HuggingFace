{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the business problem\n",
    "\n",
    "Amazon sells thousands of different products, and I'd like to understand the customer by collecting reviews posted online.\n",
    "\n",
    "\n",
    "> Given an English Language product review for shoes, I want to predict a star rating between 1 and 5 to understand how satisfied customers are with their purchase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating the business problem as a Machine Learning Problem\n",
    "\n",
    "Here, I want to train a *multi-class model* for product reviews. The model should predict accurately the probabilities for each one of the five star ratings.\n",
    "\n",
    "Possible metrics for evaluating the models are *F1 Score* and *Accuracy Score*\n",
    "\n",
    "\n",
    "Thus the definition of the ML problem is **Given a test set for English Language Product reviews, I want to classify each review according to star ratings b/n 1 and 5, with an accuracy of at least xx% and an F1 Score of atleast 0.yy**\n",
    "\n",
    "> The baseline metrics can be provided by the stakeholders or set via a statistical method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning: looking for models\n",
    "\n",
    "Take knowledge learnt from one task into a related domain. Here, I search the huggingface website for text classification or sentiment analysis models I can apply to this multi-class problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, DistilBertTokenizer, DistilBertForSequenceClassification, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration juliensimon--amazon-shoe-reviews-2085fc7afbd18449\n",
      "Found cached dataset parquet (C:/Users/INNO/.cache/huggingface/datasets/juliensimon___parquet/juliensimon--amazon-shoe-reviews-2085fc7afbd18449/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 346.18it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "am_shoe_df = load_dataset(\"juliensimon/amazon-shoe-reviews\")\n",
    "\n",
    "# am_shoe_df.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    18044\n",
       "2    18039\n",
       "0    18004\n",
       "1    17980\n",
       "3    17933\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to pandas df (you can use the to_pandas method)\n",
    "amazon_shoes_train_df =  pd.DataFrame(am_shoe_df['train'])\n",
    "\n",
    "# Check the value count of each rating (watchout for class-imbalance)\n",
    "amazon_shoes_train_df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    2067\n",
       "1    2020\n",
       "0    1996\n",
       "2    1961\n",
       "4    1956\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to pandas df (you can use the to_pandas method)\n",
    "amazon_shoes_test_df =  pd.DataFrame(am_shoe_df['test'])\n",
    "\n",
    "# Check the value count of each rating (watch out for class-imbalance)\n",
    "amazon_shoes_test_df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = Dataset.from_pandas(amazon_shoes_train_df, preserve_index=False)\n",
    "\n",
    "test_df = Dataset.from_pandas(amazon_shoes_test_df, preserve_index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Deploying Models\n",
    "\n",
    "In this notebook, I train a model locally, and also deploy to Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id_1 = 'distilbert-base-uncased'\n",
    "base_model_id_2 = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "\n",
    "epochs = 1\n",
    "# 5 labels\n",
    "num_labels = 5\n",
    "learning_rate = 5e-5\n",
    "training_batch_size = 32\n",
    "eval_batch_size = 64\n",
    "save_strategy = 'no'\n",
    "save_steps = 1000\n",
    "\n",
    "\n",
    "output_data_dir = './output'\n",
    "model_dir = './model'\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {'accuracy_score': accuracy, 'f1': f1, 'recall': recall, 'precision': precision}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Download the model and slap a classification tag of 5 labels at the end\n",
    "model_1 =  AutoModelForSequenceClassification.from_pretrained(base_model_id_1, num_labels=num_labels)\n",
    "# Convert natural language steps to integer for training\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained(base_model_id_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Download the model and slap a classification tag of 5 labels at the end\n",
    "model_2 =  AutoModelForSequenceClassification.from_pretrained(base_model_id_2, num_labels=num_labels, ignore_mismatched_sizes=True)\n",
    "# Convert natural language steps to integer for training\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(base_model_id_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to tokenize the datasets. Data is processed in batches using the map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:51<00:00, 51.77s/ba]\n",
      "100%|██████████| 9/9 [00:21<00:00,  2.38s/ba]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    # Convert strings to numbers\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "\n",
    "train_dataset = train_df.map(tokenize, batched=True, batch_size=len(train_df))\n",
    "test_dataset = train_df.map(tokenize, batched=True, batch_size=len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_model_id = 'mklomo/amazon-shoe-reviews'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "                                    hub_model_id=hub_model_id,          # Push the model to the remote repo\n",
    "                                    output_dir=model_dir,\n",
    "                                    num_train_epochs=epochs,\n",
    "                                    per_device_train_batch_size=training_batch_size,\n",
    "                                    per_device_eval_batch_size=eval_batch_size,\n",
    "                                    save_strategy=save_strategy,\n",
    "                                    save_steps=save_steps,\n",
    "                                    evaluation_strategy='epoch',\n",
    "                                    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "\n",
    "# Put the pieces together with the Trainer object\n",
    "\n",
    "trainer_1 = Trainer(\n",
    "                    model=model_1,\n",
    "                    args=training_args,\n",
    "                    tokenizer=tokenizer_1,\n",
    "                    compute_metrics=compute_metrics,\n",
    "                    train_dataset=train_dataset,\n",
    "                    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "\n",
    "trainer_2 = Trainer(\n",
    "                    model=model_2,\n",
    "                    args=training_args,\n",
    "                    tokenizer=tokenizer_2,\n",
    "                    compute_metrics=compute_metrics,\n",
    "                    train_dataset=train_dataset,\n",
    "                    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\INNO\\Documents\\Python Development\\Hugging-Face-Projects\\.hugging_face_venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 90000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2813\n",
      "  0%|          | 0/2813 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Train model 1\n",
    "trainer_1.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".hugging_face_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3f19eafba2e156c54ba8da3f47b7f6ac625b946f455dcef0145c02e55bf47f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
